{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from dataclasses import dataclass\n",
    "from itertools import permutations\n",
    "import json\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_choices = int(os.environ['MAX_N_CHOICES'])\n",
    "prompt_type = os.environ.get('PROMPT_TYPE', None)\n",
    "cot_style = os.environ.get('COT_STYLE', None)\n",
    "empty_mcp = eval(os.environ['EMPTY_MCP'])\n",
    "rotate_mcp = eval(os.environ['ROTATE_MCP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_ltr(idx):\n",
    "    return chr(idx + ord(\"A\"))\n",
    "\n",
    "@dataclass\n",
    "class QuestionPart:\n",
    "    text: str\n",
    "    tag: str = None\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.tag is not None:\n",
    "            return f\"{self.tag}: {self.text}\"\n",
    "        else:\n",
    "            return self.text\n",
    "\n",
    "@dataclass\n",
    "class Question:\n",
    "    parts: list\n",
    "    choices: list\n",
    "    answer_idx: int\n",
    "    task: str = None\n",
    "    prompt_type: str = None\n",
    "    cot_style: str = None\n",
    "\n",
    "    def get_n_choices(self):\n",
    "        return len(self.choices)\n",
    "\n",
    "    def get_answer_str(self):\n",
    "        return self.choices[self.answer_idx]\n",
    "\n",
    "    def _get_prompt(self, include_choices):\n",
    "        prompt = \"\"\n",
    "        for part in self.parts:\n",
    "            prompt += f\"{str(part)}\\n\"\n",
    "        if include_choices:\n",
    "            for i, choice in enumerate(self.choices):\n",
    "                prompt += f\"{idx_to_ltr(i)}. {choice}\\n\"\n",
    "        if self.prompt_type == 'COT':\n",
    "            if self.cot_style == 'STANDARD':\n",
    "                return prompt + \"Answer: Let me think step by step.\\n\"  \n",
    "            elif self.cot_style == 'EXPERT':  \n",
    "                return prompt + \"Answer: Let me think step by step as a reliability engineer.\\n\"\n",
    "            elif self.cot_style == 'INDUCTIVE':  \n",
    "                return prompt + \"Answer: Let's use step by step inductive reasoning, given the domain specific nature of the question.\\n\"\n",
    "        return prompt + \"Please output the answer in the first line.\\nAnswer:\\n\"\n",
    "    \n",
    "\n",
    "    def get_natural_prompt(self):\n",
    "        return self._get_prompt(include_choices=True)\n",
    "        # return self._get_prompt(include_choices=True)\n",
    "\n",
    "    def get_brown_prompt(self):\n",
    "        return self._get_prompt(include_choices=False)\n",
    "\n",
    "    def strong_shuffle(self):\n",
    "        if len(set(self.choices)) == 1:\n",
    "            return\n",
    "\n",
    "        answer_idx = self.answer_idx\n",
    "        answer_str = self.get_answer_str()\n",
    "        while self.choices[answer_idx] == answer_str:\n",
    "            random.shuffle(self.choices)\n",
    "            self.answer_idx = self.choices.index(answer_str)\n",
    "\n",
    "    def permute_choices(self, perm):\n",
    "        self.choices = [self.choices[i] for i in perm]\n",
    "        self.answer_idx = perm.index(self.answer_idx)\n",
    "\n",
    "class Exemplar(Question):\n",
    "\n",
    "    def get_natural_prompt(self):\n",
    "        prompt = super().get_natural_prompt()\n",
    "        answer_ltr = idx_to_ltr(self.answer_idx)\n",
    "        return f\"{prompt} {answer_ltr}\"\n",
    "\n",
    "    def get_brown_prompt(self):\n",
    "        prompt = super().get_brown_prompt()\n",
    "        return f\"{prompt} {self.get_answer_str()}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Passage = [\"Read the given passage, question and select the most appropriate answer by indicating the associated letter.\\nPassage: Failure Mode and Effect Analysis (FMEA) or Failure Mode, Effects, and Criticality Analysis (FMECA) are conducted to identify anticipated faults, symptoms, and potential parameters that indicate the presence or occurrence of faults. \\n\\n The FMEA and FMECA audits provide information on the parameters to be measured for specific failure modes. These parameters typically indicate a fault condition through either an increase or decrease in a particular variable measured by sensors. \\n\\n You will receive a specific failure mode and a list of parameters being monitored via sensor variables.\\n\"]\n",
    "\n",
    "PositiveQuestionsFailures = [\n",
    "    \"For {asset_class}, if a failure event {failure_mode} occurs, which sensor out of the choices is the most relevant sensor regarding the occurrence of the failure event?\",\n",
    "    \"When a {asset_class} has {failure_mode}, which sensor out of the choices should be the sensor to be monitored for this failure if I want to build an anomaly detection model?\",\n",
    "    \"Which sensor out of the choices can indicate the presence of {failure_mode} in asset {asset_class}?\",\n",
    "    \"In {asset_class}, when {failure_mode} occurs, which sensor from the choices is most critical in detecting the occurrence of the failure event?\",\n",
    "    \"Which sensor among the choices best correlates with the presence of {failure_mode} in asset {asset_class}?\",\n",
    "    \"For {asset_class}, if {failure_mode} happens, which sensor should be prioritized for monitoring this specific failure?\",\n",
    "    \"Which sensor out of the choices provides the strongest indication of {failure_mode} in {asset_class}?\",\n",
    "    \"When dealing with {failure_mode} in {asset_class}, which sensor among the choices has the highest relevance in detecting this issue?\",\n",
    " ]\n",
    "\n",
    "NegationQuestionsFailures = [\n",
    "    \"For {asset_class}, if a failure event {failure_mode} occurs, which sensor out of the choices is not relevant regarding the occurrence of the failure event?\",\n",
    "    \"When a {asset_class} has {failure_mode}, which sensor out of the choices should not be the sensor to be monitored for this failure if I want to build an anomaly detection model?\",\n",
    "    \"Which sensor out of the choices does not indicate the presence of {failure_mode} in asset {asset_class}?\",\n",
    "    \"Which sensor from the choices does not contribute significantly to detecting {failure_mode} in {asset_class}?\",\n",
    "    \"For {asset_class}, if {failure_mode} occurs, which sensor among the choices is least likely to be relevant in identifying this failure?\",\n",
    "    \"When considering {failure_mode} in {asset_class}, which sensor should be disregarded from the choices for monitoring this failure?\",\n",
    "    \"Which sensor out of the choices is not effective in indicating the presence of {failure_mode} in {asset_class}?\",\n",
    "    \"In {asset_class}, which sensor among the choices is least useful for detecting {failure_mode}?\",\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PositiveQuestionsSensors = [\n",
    "    \"In the context of {asset_class}, which failure mode is most relevant when {sensor} shows abnormal readings?\",\n",
    "    \"What is the most relevant failure mode for {asset_class} if {sensor} exhibits abnormal readings?\",\n",
    "    \"Which failure mode should be considered for {asset_class} when abnormal readings is detected by {sensor}?\",\n",
    "    \"For {asset_class}, which failure mode is pertinent if {sensor} registers abnormal readings?\",\n",
    "    \"When {sensor} in {asset_class} displays abnormal readings, which failure mode is the most applicable?\",\n",
    "    \"If {sensor} in {asset_class} shows abnormal readings, which failure mode is most significant?\",\n",
    "    \"Which failure mode is most relevant for {asset_class} if there are abnormal readings from {sensor}?\",\n",
    "    \"For {asset_class}, what is the key failure mode when {sensor} has abnormal readings?\",\n",
    "    \"In {asset_class}, which failure mode is most important if {sensor} shows abnormal readings?\",\n",
    "    \"When {sensor} detects abnormal readings in {asset_class}, which failure mode is the most relevant?\"\n",
    "]\n",
    "\n",
    "NegationQuestionsSensors = [\n",
    "    \"In the context of {asset_class}, which failure event is not relevant when the sensor {sensor} shows an abnormal reading?\",\n",
    "    \"What is the irrelevant failure event for {asset_class} if the sensor {sensor} exhibits an abnormal reading?\",\n",
    "    \"Which failure event should be excluded for {asset_class} when an abnormal reading is detected by the sensor {sensor}?\",\n",
    "    \"For {asset_class}, which failure event is not pertinent if the sensor {sensor} registers an abnormal reading?\",\n",
    "    \"When the sensor {sensor} in {asset_class} displays an abnormal reading, which failure event is not applicable?\",\n",
    "    \"If the sensor {sensor} in {asset_class} shows an abnormal reading, which failure event is insignificant?\",\n",
    "    \"Which failure event is irrelevant for {asset_class} if there is an abnormal reading from the sensor {sensor}?\",\n",
    "    \"For {asset_class}, what is the non-relevant failure event when the sensor {sensor} has an abnormal reading?\",\n",
    "    \"In {asset_class}, which failure event is unimportant if the sensor {sensor} shows an abnormal reading?\",\n",
    "    \"When an abnormal reading is detected by the sensor {sensor} in {asset_class}, which failure event is not relevant?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/single_answer_negatives.json', 'r') as f:\n",
    "    data_irrelevant = json.loads(f.read())\n",
    "with open('data/single_answer.json', 'r') as f:\n",
    "    data_relevant = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for relevant multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_relevant = []\n",
    "for i, item in enumerate(data_relevant):\n",
    "    sampled_passage = random.choice(Passage)\n",
    "    if 'failure_mode' in item:\n",
    "        sampled_question = random.choice(PositiveQuestionsFailures)\n",
    "        sampled_question_idx = \"PositiveQuestionsFailures_\" + str(PositiveQuestionsFailures.index(sampled_question))\n",
    "        prepared_question = sampled_question.replace('{failure_mode}', item['failure_mode'])\n",
    "    else:\n",
    "        sampled_question = random.choice(PositiveQuestionsSensors)\n",
    "        sampled_question_idx = \"PositiveQuestionsSensors_\" + str(PositiveQuestionsSensors.index(sampled_question))\n",
    "        prepared_question = sampled_question.replace('{sensor}', item['sensor'])\n",
    "    prepared_question = prepared_question.replace('{asset_class}', item['asset_class'])\n",
    "    parts = [\n",
    "        QuestionPart(text=sampled_passage, tag=None),\n",
    "        QuestionPart(text=prepared_question, tag=\"Question\"),\n",
    "    ]\n",
    "    qa_pairs = np.array([(key, item['mc_targets'][key]) for key in item['mc_targets']])\n",
    "    answer_idx = np.where(qa_pairs[:,1].astype(int)==1)[0][0]\n",
    "    incorrect_idxs = np.where(qa_pairs[:,1].astype(int)==0)[0]\n",
    "    if len(incorrect_idxs) == 0:\n",
    "        continue\n",
    "    n_choices = min(max_n_choices-1, len(incorrect_idxs))\n",
    "    sampled_incorrect_idxs = np.random.choice(incorrect_idxs, n_choices, replace=False)\n",
    "    incorrect_answers = [qa_pairs[idx] for idx in sampled_incorrect_idxs]\n",
    "    correct_answer = np.array([qa_pairs[answer_idx]])\n",
    "    all_answers = np.concatenate([correct_answer, incorrect_answers])\n",
    "    # shuffle, so the answer is in a random position\n",
    "    np.random.shuffle(all_answers)\n",
    "    answer_idx = np.argmax(all_answers[:, 1].astype(int))\n",
    "    n_rotations = 1\n",
    "    if empty_mcp:\n",
    "        # used for bias test\n",
    "        all_answers[:, 0] = ''\n",
    "    q = Question(parts=parts, choices=all_answers[:, 0].tolist(), answer_idx=answer_idx, \n",
    "                 prompt_type=prompt_type, cot_style=cot_style)\n",
    "    if rotate_mcp:\n",
    "        n_rotations = q.get_n_choices()\n",
    "    rotations = list(range(len(all_answers)))\n",
    "    for _ in range(n_rotations):\n",
    "        q.permute_choices(rotations)\n",
    "        item_dict = {\n",
    "            'input': q.get_natural_prompt(), \n",
    "            'output': q.get_answer_str(), \n",
    "            \"relevancy\": item[\"question\"],\n",
    "            \"sampled_question_idx\": sampled_question_idx,\n",
    "            'question_type': 'mcp1_positive',\n",
    "            'question_idx': i,\n",
    "            'answer_letter': idx_to_ltr(answer_idx),\n",
    "            'rotation': rotations\n",
    "        }\n",
    "        res_relevant.append(item_dict)\n",
    "        rotations = rotations[-1:] + rotations[:-1]\n",
    "        answer_idx = (answer_idx + 1) % len(rotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for irrelevant multiple choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_irrelevant = []\n",
    "for i, item in enumerate(data_irrelevant):\n",
    "    sampled_passage = random.choice(Passage)\n",
    "    if 'failure_mode' in item:\n",
    "        sampled_question = random.choice(NegationQuestionsFailures)\n",
    "        sampled_question_idx = \"NegationQuestionsFailures_\" + str(NegationQuestionsFailures.index(sampled_question))\n",
    "        prepared_question = sampled_question.replace('{failure_mode}', item['failure_mode'])\n",
    "    else:\n",
    "        sampled_question = random.choice(NegationQuestionsSensors)\n",
    "        sampled_question_idx = \"NegationQuestionsSensors_\" + str(NegationQuestionsSensors.index(sampled_question))\n",
    "        prepared_question = sampled_question.replace('{sensor}', item['sensor'])\n",
    "    prepared_question = prepared_question.replace('{asset_class}', item['asset_class'])\n",
    "    parts = [\n",
    "        QuestionPart(text=sampled_passage, tag=None),\n",
    "        QuestionPart(text=prepared_question, tag=\"Question\"),\n",
    "    ]\n",
    "    qa_pairs = np.array([(key, item['mc_targets'][key]) for key in item['mc_targets']])\n",
    "    answer_idx = np.where(qa_pairs[:,1].astype(int)==1)[0][0]\n",
    "    incorrect_idxs = np.where(qa_pairs[:,1].astype(int)==0)[0]\n",
    "    if len(incorrect_idxs) == 0:\n",
    "        continue\n",
    "    n_choices = min(max_n_choices-1, len(incorrect_idxs))\n",
    "    sampled_incorrect_idxs = np.random.choice(incorrect_idxs, n_choices, replace=False)\n",
    "    incorrect_answers = [qa_pairs[idx] for idx in sampled_incorrect_idxs]\n",
    "    correct_answer = np.array([qa_pairs[answer_idx]])\n",
    "    all_answers = np.concatenate([correct_answer, incorrect_answers])\n",
    "    # shuffle, so the answer is in a random position\n",
    "    np.random.shuffle(all_answers)\n",
    "    answer_idx = np.argmax(all_answers[:, 1].astype(int))\n",
    "    if empty_mcp:\n",
    "        # used for bias test\n",
    "        all_answers[:, 0] = ''\n",
    "    q = Question(parts=parts, choices=all_answers[:, 0].tolist(), answer_idx=answer_idx, \n",
    "                 prompt_type=prompt_type, cot_style=cot_style)\n",
    "    if rotate_mcp:\n",
    "        n_rotations = q.get_n_choices()\n",
    "    rotations = list(range(len(all_answers)))\n",
    "    for _ in range(n_rotations):\n",
    "        q.permute_choices(rotations)\n",
    "        item_dict = {\n",
    "            'input': q.get_natural_prompt(), \n",
    "            'output': q.get_answer_str(), \n",
    "            'sampled_question_idx': sampled_question_idx,\n",
    "            \"relevancy\": item[\"question\"],\n",
    "            'question_type': 'mcp1_negative',\n",
    "            'question_idx': i,\n",
    "            'answer_letter': idx_to_ltr(answer_idx),\n",
    "            'rotation': rotations\n",
    "        }\n",
    "        res_irrelevant.append(item_dict)\n",
    "        rotations = rotations[-1:] + rotations[:-1]\n",
    "        answer_idx = (answer_idx + 1) % len(rotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the given passage, question and select the most appropriate answer by indicating the associated letter.\n",
      "Passage: Failure Mode and Effect Analysis (FMEA) or Failure Mode, Effects, and Criticality Analysis (FMECA) are conducted to identify anticipated faults, symptoms, and potential parameters that indicate the presence or occurrence of faults. \n",
      "\n",
      " The FMEA and FMECA audits provide information on the parameters to be measured for specific failure modes. These parameters typically indicate a fault condition through either an increase or decrease in a particular variable measured by sensors. \n",
      "\n",
      " You will receive a specific failure mode and a list of parameters being monitored via sensor variables.\n",
      "\n",
      "Question: For electric motor, if a failure event rotor windings fault occurs, which sensor out of the choices is the most relevant sensor regarding the occurrence of the failure event?\n",
      "A. partial discharge\n",
      "B. resistance\n",
      "C. oil debris\n",
      "D. current\n",
      "E. voltage\n",
      "Please output the answer in the first line.\n",
      "Answer:\n",
      "\n",
      "current\n"
     ]
    }
   ],
   "source": [
    "print(res_relevant[0]['input'])\n",
    "print(res_relevant[0]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the given passage, question and select the most appropriate answer by indicating the associated letter.\n",
      "Passage: Failure Mode and Effect Analysis (FMEA) or Failure Mode, Effects, and Criticality Analysis (FMECA) are conducted to identify anticipated faults, symptoms, and potential parameters that indicate the presence or occurrence of faults. \n",
      "\n",
      " The FMEA and FMECA audits provide information on the parameters to be measured for specific failure modes. These parameters typically indicate a fault condition through either an increase or decrease in a particular variable measured by sensors. \n",
      "\n",
      " You will receive a specific failure mode and a list of parameters being monitored via sensor variables.\n",
      "\n",
      "Question: For electric motor, if rotor windings fault occurs, which sensor among the choices is least likely to be relevant in identifying this failure?\n",
      "A. cooling gas\n",
      "B. current\n",
      "C. vibration\n",
      "D. power\n",
      "E. voltage\n",
      "Please output the answer in the first line.\n",
      "Answer:\n",
      "\n",
      "voltage\n"
     ]
    }
   ],
   "source": [
    "print(res_irrelevant[0]['input'])\n",
    "print(res_irrelevant[0]['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_fname = 'data/prepared_mcp1'\n",
    "if prompt_type is not None and not empty_mcp:\n",
    "    result_fname = f'data/prepared_mcp1_{prompt_type.lower()}'\n",
    "if prompt_type == 'COT':\n",
    "    result_fname += f'_{cot_style.lower()}'\n",
    "if prompt_type is not None and empty_mcp:\n",
    "    result_fname += '_empty'\n",
    "if prompt_type is not None and rotate_mcp:\n",
    "    result_fname += '_rotate'\n",
    "result_fname += '.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = res_relevant + res_irrelevant\n",
    "if empty_mcp:\n",
    "    # for empty options sample 50\n",
    "    sensor_five_options = [\n",
    "        item for item in res_relevant + res_irrelevant if len(item['rotation']) == 5 and 'for_sensor' in item['relevancy']\n",
    "    ]\n",
    "    mode_five_options = [\n",
    "        item for item in res_relevant + res_irrelevant if len(item['rotation']) == 5 and 'for_failure_mode' in item['relevancy']\n",
    "    ]\n",
    "    sampled_sensor = random.sample(sensor_five_options, 50)\n",
    "    sampled_mode = random.sample(mode_five_options, 50)\n",
    "    result = sampled_sensor + sampled_mode\n",
    "elif rotate_mcp:\n",
    "    all_five_option_relevant = [item['question_idx'] for item in res_relevant if item['rotation'] == list(range(5))]\n",
    "    all_five_option_irrelevant = [item['question_idx'] for item in res_irrelevant if item['rotation'] == list(range(5))]\n",
    "    sampled_idx_relevant = random.sample(all_five_option_relevant, 50)\n",
    "    sampled_idx_irrelevant = random.sample(all_five_option_irrelevant, 50)\n",
    "    sampled_five_option_relevant = [item for item in res_relevant if item['question_idx'] in sampled_idx_relevant]\n",
    "    sampled_five_option_irrelevant = [item for item in res_irrelevant if item['question_idx'] in sampled_idx_irrelevant]\n",
    "    result = sampled_five_option_relevant + sampled_five_option_irrelevant\n",
    "with open(result_fname, 'w') as f:\n",
    "    json.dump(result, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(923, 1744, 2667)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res_relevant), len(res_irrelevant), len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
