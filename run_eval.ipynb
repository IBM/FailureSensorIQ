{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acd01bf7-47cb-4c40-9496-c2b62fb83ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a1db6ab-ccf4-49b2-956c-4338d09e832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('perteval')\n",
    "sys.path.append('uq_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63c915d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from utils import get_requests, get_results\n",
    "from perteval.perteval_end_to_end import get_perteval_results\n",
    "# from uq_score.uq_end_to_end import run_uq\n",
    "from llm_uncertainty_bench.uq_end_to_end import run_uq_benchmark\n",
    "import datetime\n",
    "import perteval.transition_analysis as tas\n",
    "import time\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978b45b7-8727-411f-9f99-4e36bcc2ddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kill: not enough arguments\n"
     ]
    }
   ],
   "source": [
    "# try killing old processes that utilize the gpu\n",
    "try:\n",
    "    subprocess.run([\"nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9\"], shell=True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4108b98f-2ec7-406a-b334-56ddf21f36c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e7f8b748b540cdaf52998e77cbabef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 30 files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e93dc146e8047428db716b4196c87d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)val_request_False_bfloat16_Original.json:   0%|          | 0.00/304 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u/modelfactory/.cache/huggingface/hub/datasets--cc4718--requests/snapshots/8defb02c7b09340087c6371dfecd995c950fb309\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57bff16a3a84259842eff9edb7281d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 27 files:   0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "820ec819bdc74d59a89a7fbb2fcfe865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)lts_mistralai--Magistral-Small-2506.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/u/modelfactory/.cache/huggingface/hub/datasets--cc4718--results/snapshots/23863448d4f952ed673a85a288fdb606b0c0c36c/demo-leaderboard/gpt2-demo\n",
      "models to evaluate:\n",
      "{'ibm-granite/granite-3.1-8b-instruct'}\n",
      "dataset:\n",
      "full\n"
     ]
    }
   ],
   "source": [
    "if len(sys.argv) == 1 or sys.argv[1] == '-f':\n",
    "    # -f is to avoid the case that this runs as ipynb\n",
    "    all_request_models = get_requests('cc4718/requests')\n",
    "    all_result_models = get_results('cc4718/results')\n",
    "    models_todo = set(all_request_models) - set(all_result_models)\n",
    "    # dataset can be full/sample\n",
    "    dataset = 'full'\n",
    "else:\n",
    "    # huggingface models\n",
    "    models_todo = [sys.argv[1]]\n",
    "    # dataset full/sample\n",
    "    dataset = sys.argv[2]\n",
    "print('models to evaluate:')\n",
    "print(models_todo)\n",
    "print('dataset:')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8220d2-3eff-49a1-95dd-0dc02ac31161",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for model_name in models_todo:\n",
    "    # perteval\n",
    "    original_log_path, test_path = get_perteval_results(model_name, \n",
    "                                                        mode='original', \n",
    "                                                        cot='cot_standard', \n",
    "                                                        dataset=dataset)\n",
    "    print('Finished perteval on original data')\n",
    "    # wait a few seconds to kill vllm and spin up a new\n",
    "    time.sleep(10)\n",
    "    perturb_log_path, _ = get_perteval_results(model_name, \n",
    "                                               mode='perturb', \n",
    "                                               cot='cot_standard', \n",
    "                                               dataset=dataset)\n",
    "    print('Finished perteval on perturbed data')\n",
    "    original, perturb, consist = tas.transition_analysis(original_log_path, \n",
    "                                                         perturb_log_path, \n",
    "                                                         subjects=[\"failure_mode_sensor_analysis\"])\n",
    "    asset_scores = tas.get_record_id_for_correct_answer(original_log_path, \n",
    "                                                        dimention='asset_name', \n",
    "                                                        fdata_path=test_path)\n",
    "    relevancy_scores = tas.get_record_id_for_correct_answer(original_log_path, \n",
    "                                                            dimention='relevancy', \n",
    "                                                            fdata_path=test_path)\n",
    "    # uq bench\n",
    "    print('Running LLM Uncertainty Bench')\n",
    "    uq_scores = run_uq_benchmark(model_name, prompt_type='chat', dataset=dataset)\n",
    "    if dataset == 'sample':\n",
    "        acc_sel_key = 'relevant_sensor_for_failure_mode'\n",
    "        acc_el_key = 'irrelevant_sensor_for_failure_mode'\n",
    "    else:\n",
    "        acc_sel_key = 'relevant_sensors_for_failure_mode'\n",
    "        acc_el_key = 'irrelevant_sensors_for_failure_mode'\n",
    "    result_dict = {\n",
    "        \"config\": {\n",
    "            \"model_dtype\": \"torch.bfloat16\", \n",
    "            \"model_name\": model_name,\n",
    "            \"model_sha\": \"main\"\n",
    "        },\n",
    "        \"results\": {\n",
    "            \"acc_overall\": {\n",
    "                \"acc\": original\n",
    "            },\n",
    "            \"acc_sel\": {\n",
    "                \"acc_sel\": relevancy_scores[acc_sel_key]\n",
    "            },\n",
    "            \"acc_el\": {\n",
    "                \"acc_el\": relevancy_scores[acc_el_key]\n",
    "            },\n",
    "            \"acc_perturb\": {\n",
    "                \"perturb_score\": perturb\n",
    "            },\n",
    "            \"score_consistency\": {\n",
    "                \"consist_score\": consist\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    print(uq_scores)\n",
    "    for k, v in uq_scores.items():\n",
    "        result_dict['results'][k] = {k: v}\n",
    "    for asset in asset_scores:\n",
    "        if not asset:\n",
    "            asset_lower = 'other'\n",
    "        else:\n",
    "            asset_lower = asset.lower().replace(' ', '_')\n",
    "        result_dict['results'][f'acc_{asset_lower}'] = {f'acc_{asset_lower}': asset_scores[asset]}\n",
    "    out_model_name = model_name.replace('/', '--')\n",
    "    out_fname = f'results/demo-leaderboard/gpt2-demo/results_{out_model_name}.json'\n",
    "    with open(out_fname, 'w') as f:\n",
    "        f.write(json.dumps(result_dict))\n",
    "    print(f'all results written to {out_fname}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd46298-1e7a-4d8f-b28b-5baa1b84f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi | grep 'python' | awk '{ print $5 }' | xargs -n1 kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41740431-b149-451a-831d-c9077adde2c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
